{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c08c6b-aaec-470e-ba23-0b6a1e934a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputdata = pd.read_csv(\"./generated/atp_matches_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8d811-2ad9-4b4e-8ce6-517ca2ca0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "#https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#select the features one wants to extract\n",
    "feature_cols = ['winner_id', 'loser_id','surface_clay', 'surface_hard','surface_carpet','surface_grass','hand_l','hand_r', 'round_RR', 'round_R128', 'round_R64', 'round_R32', 'round_R16', 'round_QF','round_SF', 'round_F']\n",
    "\n",
    "#In X are extracted features, aka feature set\n",
    "X = inputdata.loc[:, feature_cols]\n",
    "#These are the output labels to train with.\n",
    "y = inputdata['winner_id']\n",
    "\n",
    "#Here we split our labels and feature set in 80%/20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) \n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train=sc_X.fit_transform(X_train)\n",
    "X_test=sc_X.fit_transform(X_test)\n",
    "#Select the classifier and the parameters\n",
    "clf = MLPClassifier(solver='lbfgs', activation='relu', hidden_layer_sizes=(1500,1000,500),max_iter=1500, random_state=1)\n",
    "\n",
    "#Fit the classifier, the model gets trained.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7769b86c-b024-4d6d-86c8-338e87ce2a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      101962       0.00      0.00      0.00         3\n",
      "      102148       1.00      0.12      0.22         8\n",
      "      102434       0.00      0.00      0.00         4\n",
      "      102512       0.00      0.00      0.00         2\n",
      "      102558       0.00      0.00      0.00         1\n",
      "      102563       0.00      0.00      0.00         5\n",
      "      102567       0.00      0.00      0.00         1\n",
      "      102630       0.00      0.00      0.00         1\n",
      "      102703       0.00      0.00      0.00         4\n",
      "      102780       0.00      0.00      0.00         1\n",
      "      102783       0.00      0.00      0.00         9\n",
      "      102788       0.00      0.00      0.00         1\n",
      "      102821       0.00      0.00      0.00         1\n",
      "      102839       0.00      0.00      0.00         3\n",
      "      102845       0.10      0.57      0.17         7\n",
      "      102860       0.00      0.00      0.00         6\n",
      "      102863       0.00      0.00      0.00         2\n",
      "      102887       0.07      1.00      0.13         1\n",
      "      102967       0.00      0.00      0.00         7\n",
      "      103017       0.00      0.00      0.00         9\n",
      "      103077       0.00      0.00      0.00         1\n",
      "      103084       0.00      0.00      0.00         5\n",
      "      103096       0.00      0.00      0.00         5\n",
      "      103103       0.00      0.00      0.00         1\n",
      "      103163       0.00      0.00      0.00         8\n",
      "      103169       0.00      0.00      0.00         7\n",
      "      103174       0.00      0.00      0.00         1\n",
      "      103181       0.00      0.00      0.00         5\n",
      "      103188       0.00      0.00      0.00         1\n",
      "      103204       0.00      0.00      0.00         1\n",
      "      103206       0.00      0.00      0.00         2\n",
      "      103228       0.00      0.00      0.00         2\n",
      "      103252       0.00      0.00      0.00         1\n",
      "      103285       0.00      0.00      0.00        10\n",
      "      103286       0.00      0.00      0.00         1\n",
      "      103294       0.00      0.00      0.00         1\n",
      "      103333       0.32      0.78      0.45         9\n",
      "      103344       0.00      0.00      0.00         8\n",
      "      103373       0.00      0.00      0.00         2\n",
      "      103393       0.00      0.00      0.00         1\n",
      "      103395       0.00      0.00      0.00         1\n",
      "      103401       0.00      0.00      0.00         2\n",
      "      103428       0.00      0.00      0.00         4\n",
      "      103429       0.00      0.00      0.00         1\n",
      "      103444       0.00      0.00      0.00         0\n",
      "      103451       0.00      0.00      0.00         1\n",
      "      103454       0.00      0.00      0.00         2\n",
      "      103470       0.00      0.00      0.00         1\n",
      "      103472       0.00      0.00      0.00         1\n",
      "      103484       0.12      0.06      0.08        17\n",
      "      103490       0.00      0.00      0.00         4\n",
      "      103498       0.17      0.14      0.15         7\n",
      "      103506       0.00      0.00      0.00         1\n",
      "      103507       0.11      0.40      0.17         5\n",
      "      103521       0.00      0.00      0.00         1\n",
      "      103523       0.00      0.00      0.00         1\n",
      "      103534       0.00      0.00      0.00         2\n",
      "      103535       0.00      0.00      0.00         2\n",
      "      103553       0.00      0.00      0.00         1\n",
      "      103566       0.00      0.00      0.00         3\n",
      "      103573       0.00      0.00      0.00         2\n",
      "      103580       0.00      0.00      0.00         3\n",
      "      103582       0.00      0.00      0.00         2\n",
      "      103598       0.00      0.00      0.00         3\n",
      "      103602       0.11      0.18      0.14        11\n",
      "      103632       0.00      0.00      0.00         1\n",
      "      103638       0.00      0.00      0.00         1\n",
      "      103656       0.18      0.50      0.26         6\n",
      "      103675       0.00      0.00      0.00         1\n",
      "      103693       0.00      0.00      0.00         1\n",
      "      103694       0.00      0.00      0.00         5\n",
      "      103701       0.00      0.00      0.00         1\n",
      "      103720       0.43      0.60      0.50         5\n",
      "      103722       0.00      0.00      0.00         6\n",
      "      103781       0.00      0.00      0.00        11\n",
      "      103786       0.29      0.26      0.28        19\n",
      "      103794       0.00      0.00      0.00         1\n",
      "      103808       0.00      0.00      0.00         9\n",
      "      103812       0.00      0.00      0.00         7\n",
      "      103813       0.00      0.00      0.00         7\n",
      "      103819       0.23      0.47      0.30        15\n",
      "      103821       0.00      0.00      0.00         2\n",
      "      103823       0.00      0.00      0.00         3\n",
      "      103835       0.00      0.00      0.00         4\n",
      "      103851       0.00      0.00      0.00         2\n",
      "      103852       0.00      0.00      0.00         8\n",
      "      103857       0.11      0.20      0.14         5\n",
      "      103868       0.00      0.00      0.00         1\n",
      "      103888       0.33      0.25      0.29         8\n",
      "      103898       0.00      0.00      0.00         6\n",
      "      103900       0.23      0.18      0.20        17\n",
      "      103908       0.14      0.75      0.24         4\n",
      "      103909       0.00      0.00      0.00         2\n",
      "      103917       0.00      0.00      0.00         3\n",
      "      103970       0.75      0.50      0.60        12\n",
      "      103976       0.00      0.00      0.00         2\n",
      "      103990       0.40      1.00      0.57         8\n",
      "      104003       0.00      0.00      0.00         0\n",
      "      104019       0.00      0.00      0.00         3\n",
      "      104022       0.33      0.43      0.38         7\n",
      "      104025       0.00      0.00      0.00         3\n",
      "      104035       0.00      0.00      0.00         2\n",
      "      104053       0.44      0.89      0.59         9\n",
      "      104056       0.00      0.00      0.00         2\n",
      "      104059       0.00      0.00      0.00         1\n",
      "      104068       0.00      0.00      0.00         9\n",
      "      104076       0.29      0.33      0.31         6\n",
      "      104081       0.00      0.00      0.00         0\n",
      "      104098       0.50      0.67      0.57         9\n",
      "      104112       0.00      0.00      0.00         1\n",
      "      104122       0.00      0.00      0.00         1\n",
      "      104154       0.00      0.00      0.00         3\n",
      "      104160       0.00      0.00      0.00         1\n",
      "      104162       0.00      0.00      0.00         1\n",
      "      104180       0.00      0.00      0.00         1\n",
      "      104198       0.33      0.67      0.44         6\n",
      "      104214       0.39      0.58      0.47        12\n",
      "      104216       0.00      0.00      0.00         3\n",
      "      104219       0.00      0.00      0.00         4\n",
      "      104229       0.00      0.00      0.00         2\n",
      "      104247       0.00      0.00      0.00         2\n",
      "      104250       0.00      0.00      0.00         1\n",
      "      104251       0.00      0.00      0.00         1\n",
      "      104259       0.06      0.20      0.09         5\n",
      "      104262       0.00      0.00      0.00         1\n",
      "      104268       0.00      0.00      0.00         5\n",
      "      104269       0.25      0.18      0.21        17\n",
      "      104273       0.00      0.00      0.00         1\n",
      "      104299       0.00      0.00      0.00         2\n",
      "      104312       0.11      0.17      0.13        12\n",
      "      104314       0.00      0.00      0.00         1\n",
      "      104327       0.00      0.00      0.00         4\n",
      "      104330       0.00      0.00      0.00         1\n",
      "      104332       0.00      0.00      0.00         3\n",
      "      104338       0.00      0.00      0.00        11\n",
      "      104339       0.00      0.00      0.00         8\n",
      "      104349       0.00      0.00      0.00         2\n",
      "      104368       0.00      0.00      0.00         1\n",
      "      104371       0.00      0.00      0.00         4\n",
      "      104386       0.27      0.33      0.30         9\n",
      "      104395       0.00      0.00      0.00         1\n",
      "      104402       0.00      0.00      0.00         1\n",
      "      104417       0.09      0.18      0.12        11\n",
      "      104424       0.00      0.00      0.00         1\n",
      "      104433       0.00      0.00      0.00         1\n",
      "      104467       0.00      0.00      0.00         1\n",
      "      104468       0.11      0.09      0.10        22\n",
      "      104476       0.00      0.00      0.00         1\n",
      "      104500       0.00      0.00      0.00         1\n",
      "      104501       0.00      0.00      0.00         1\n",
      "      104523       0.00      0.00      0.00         2\n",
      "      104527       0.06      0.18      0.10        11\n",
      "      104534       0.00      0.00      0.00         3\n",
      "      104535       0.00      0.00      0.00         1\n",
      "      104542       0.00      0.00      0.00        10\n",
      "      104545       0.00      0.00      0.00         3\n",
      "      104559       0.00      0.00      0.00         7\n",
      "      104571       0.00      0.00      0.00         4\n",
      "      104579       0.00      0.00      0.00         1\n",
      "      104583       0.00      0.00      0.00         1\n",
      "      104586       0.00      0.00      0.00         1\n",
      "      104589       0.00      0.00      0.00         6\n",
      "      104593       0.00      0.00      0.00         3\n",
      "      104597       0.02      0.29      0.04         7\n",
      "      104607       0.00      0.00      0.00        16\n",
      "      104618       0.00      0.00      0.00         1\n",
      "      104620       0.00      0.00      0.00         5\n",
      "      104639       0.00      0.00      0.00         4\n",
      "      104655       0.00      0.00      0.00         2\n",
      "      104660       0.00      0.00      0.00         4\n",
      "      104665       0.00      0.00      0.00         1\n",
      "      104676       0.00      0.00      0.00         2\n",
      "      104678       0.00      0.00      0.00         4\n",
      "      104717       0.00      0.00      0.00         1\n",
      "      104719       0.00      0.00      0.00         3\n",
      "      104721       0.00      0.00      0.00         1\n",
      "      104724       0.00      0.00      0.00         5\n",
      "      104731       0.00      0.00      0.00         7\n",
      "      104745       0.33      0.93      0.48        30\n",
      "      104755       0.00      0.00      0.00         9\n",
      "      104763       0.00      0.00      0.00         1\n",
      "      104779       0.00      0.00      0.00         1\n",
      "      104783       0.00      0.00      0.00         1\n",
      "      104792       0.00      0.00      0.00         7\n",
      "      104797       0.00      0.00      0.00         2\n",
      "      104800       0.00      0.00      0.00         1\n",
      "      104869       0.00      0.00      0.00         1\n",
      "      104871       0.00      0.00      0.00         2\n",
      "      104882       0.00      0.00      0.00         1\n",
      "      104887       0.00      0.00      0.00         1\n",
      "      104898       0.00      0.00      0.00         8\n",
      "      104918       0.00      0.00      0.00        17\n",
      "      104925       0.00      0.00      0.00        15\n",
      "      104926       0.00      0.00      0.00         8\n",
      "      104952       0.00      0.00      0.00         1\n",
      "      104958       0.00      0.00      0.00         1\n",
      "      104979       0.00      0.00      0.00         1\n",
      "      104992       0.00      0.00      0.00         1\n",
      "      104999       0.00      0.00      0.00         3\n",
      "      105011       0.00      0.00      0.00         1\n",
      "      105023       0.00      0.00      0.00         7\n",
      "      105028       0.00      0.00      0.00         3\n",
      "      105032       0.00      0.00      0.00         1\n",
      "      105052       0.00      0.00      0.00         1\n",
      "      105053       0.00      0.00      0.00         1\n",
      "      105062       0.00      0.00      0.00         1\n",
      "      105064       0.00      0.00      0.00         1\n",
      "      105095       0.00      0.00      0.00         3\n",
      "      105173       0.00      0.00      0.00         1\n",
      "      105208       0.00      0.00      0.00         6\n",
      "      105217       0.00      0.00      0.00         1\n",
      "      105221       0.00      0.00      0.00         1\n",
      "      105223       0.33      0.23      0.27        13\n",
      "      105227       0.00      0.00      0.00        12\n",
      "      105259       0.00      0.00      0.00         1\n",
      "      105311       0.00      0.00      0.00         1\n",
      "      105339       0.00      0.00      0.00         2\n",
      "      105385       0.00      0.00      0.00         4\n",
      "      105386       0.00      0.00      0.00         1\n",
      "      105394       0.00      0.00      0.00         1\n",
      "      105453       0.29      0.40      0.33         5\n",
      "      105590       0.00      0.00      0.00         2\n",
      "      105668       0.00      0.00      0.00         1\n",
      "      105992       0.00      0.00      0.00         1\n",
      "      108688       0.00      0.00      0.00         1\n",
      "      108693       0.00      0.00      0.00         1\n",
      "      108930       0.00      0.00      0.00         2\n",
      "      108945       0.00      0.00      0.00         1\n",
      "      108960       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.15       933\n",
      "   macro avg       0.04      0.06      0.04       933\n",
      "weighted avg       0.10      0.15      0.11       933\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6eb741-8a84-4cec-9b69-8f82f1866ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
